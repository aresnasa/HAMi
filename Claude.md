# HAMi æ ¸å¿ƒä»£ç åˆ—è¡¨ä¸æŠ€æœ¯ç»†èŠ‚è¯¦è§£

## é¡¹ç›®æ¦‚è¿°

HAMi (Heterogeneous AI Computing Virtualization Middleware) æ˜¯ CNCF æ²™ç›’é¡¹ç›®ï¼Œæ˜¯ä¸€ä¸ª **Kubernetes å¼‚æ„è®¾å¤‡ï¼ˆGPU/NPU/etcï¼‰è™šæ‹ŸåŒ–ä¸­é—´ä»¶**ã€‚å®ƒé€šè¿‡ä¸‰å¤§ç»„ä»¶å®ç° GPU ç­‰è®¾å¤‡çš„å…±äº«ã€éš”ç¦»ä¸è°ƒåº¦ã€‚

---

## ä¸€ã€ç³»ç»Ÿæ¶æ„æ€»è§ˆ

```
[Pod æäº¤]
    |
    v
[MutatingWebhook] â† pkg/scheduler/webhook.go
    | æ³¨å…¥è°ƒåº¦å™¨åç§°ã€ä¿®æ”¹èµ„æºè¯·æ±‚
    v
[Scheduler Extender] â† pkg/scheduler/scheduler.go
    | Filter (é€‰èŠ‚ç‚¹) + Bind (ç»‘å®šèŠ‚ç‚¹)
    v
[Device Plugin] â† cmd/device-plugin/nvidia/
    | å‘ Kubelet æ³¨å†Œè®¾å¤‡èµ„æºã€å“åº” Allocate è¯·æ±‚
    v
[vGPU Monitor] â† cmd/vGPUmonitor/
    | é€šè¿‡ mmap å…±äº«å†…å­˜ç›‘æ§å®¹å™¨ GPU ç”¨é‡ï¼Œå®ç°ä¼˜å…ˆçº§ä¸é™æµåé¦ˆ
    v
[Container] â† lib/nvidia/ld.so.preload â†’ libvgpu.so (C åº“)
    å†…æ ¸çº§æ˜¾å­˜/ç®—åŠ›éš”ç¦»
```

---

## äºŒã€æ ¸å¿ƒä»£ç æ–‡ä»¶æ¸…å•

### ğŸ”· 1. è®¾å¤‡æŠ½è±¡å±‚ï¼ˆDevice Interfaceï¼‰

#### `pkg/device/devices.go` â€” **æœ€æ ¸å¿ƒçš„æ¥å£å®šä¹‰æ–‡ä»¶**

è¿™æ˜¯æ•´ä¸ªå¼‚æ„è®¾å¤‡æŠ½è±¡çš„åŸºçŸ³ï¼š

```go
type Devices interface {
    CommonWord() string
    MutateAdmission(ctr *corev1.Container, pod *corev1.Pod) (bool, error)
    CheckHealth(devType string, n *corev1.Node) (bool, bool)
    NodeCleanUp(nn string) error
    GetResourceNames() ResourceNames
    GetNodeDevices(n corev1.Node) ([]*DeviceInfo, error)
    LockNode(n *corev1.Node, p *corev1.Pod) error
    ReleaseNodeLock(n *corev1.Node, p *corev1.Pod) error
    GenerateResourceRequests(ctr *corev1.Container) ContainerDeviceRequest
    PatchAnnotations(pod *corev1.Pod, annoinput *map[string]string, pd PodDevices) map[string]string
    ScoreNode(node *corev1.Node, podDevices PodSingleDevice, previous []*DeviceUsage, policy string) float32
    AddResourceUsage(pod *corev1.Pod, n *DeviceUsage, ctr *ContainerDevice) error
    Fit(devices []*DeviceUsage, request ContainerDeviceRequest, pod *corev1.Pod, nodeInfo *NodeInfo, allocated *PodDevices) (bool, map[string]ContainerDevices, string)
}
```

**å…³é”®æ•°æ®ç»“æ„ï¼š**

| ç±»å‹ | è¯´æ˜ |
|------|------|
| `DeviceInfo` | èŠ‚ç‚¹ä¸Šå•å—è®¾å¤‡çš„é™æ€ä¿¡æ¯ï¼ˆUUIDã€æ€»æ˜¾å­˜ã€æ ¸æ•°ã€NUMA äº²å’Œæ€§ï¼‰ |
| `DeviceUsage` | è°ƒåº¦æ—¶çš„è®¾å¤‡å®æ—¶ä½¿ç”¨çŠ¶æ€ï¼ˆå·²ç”¨æ˜¾å­˜ã€æ ¸æ•°ã€å ç”¨çš„ Podï¼‰ |
| `ContainerDevice` | åˆ†é…ç»™æŸä¸ªå®¹å™¨çš„ä¸€å—è®¾å¤‡ï¼ˆUUID + ä½¿ç”¨é‡ï¼‰ |
| `PodDevices` | `map[devType]PodSingleDevice`ï¼Œä¸€ä¸ª Pod çš„å…¨éƒ¨è®¾å¤‡åˆ†é…ç»“æœ |
| `ContainerDeviceRequest` | å®¹å™¨çš„èµ„æºè¯·æ±‚ï¼ˆæ•°é‡ Numsã€æ˜¾å­˜ Memreqã€ç®—åŠ› Coresreqï¼‰ |

---

#### `pkg/device/devices.go` â€” Annotation ç¼–è§£ç ç³»åˆ—å‡½æ•°

è®¾å¤‡åˆ†é…ä¿¡æ¯é€šè¿‡ **Kubernetes Pod/Node Annotations** ä¼ é€’ï¼Œç¼–ç åè®®æ˜¯è‡ªå®šä¹‰çš„åˆ†éš”ç¬¦æ ¼å¼ï¼š

```go
// èŠ‚ç‚¹è®¾å¤‡ç¼–ç ï¼šUUID,Count,Mem,Core,Type,Numa,Health,Index,Mode:UUID,...
func EncodeNodeDevices(dlist []*DeviceInfo) string

// Pod å®¹å™¨è®¾å¤‡ç¼–ç ï¼šUUID,Type,UsedMem,UsedCores:UUID,...
func EncodeContainerDevices(cd ContainerDevices) string

// å®¹å™¨é—´ç”¨ ; åˆ†éš”ï¼Œè®¾å¤‡é—´ç”¨ : åˆ†éš”
const OneContainerMultiDeviceSplitSymbol = ":"
const OnePodMultiContainerSplitSymbol   = ";"
```

---

### ğŸ”· 2. NVIDIA GPU è®¾å¤‡å®ç°

#### `pkg/device/nvidia/device.go` â€” **æœ€å¤æ‚çš„è®¾å¤‡å®ç°**

**å…³é”®å¸¸é‡ï¼ˆAnnotation Keysï¼‰ï¼š**
```go
const HandshakeAnnos       = "hami.io/node.nvidia.registry.time"
const RegisterAnnos        = "hami.io/node.nvidia.device-register"
const RegisterGPUPairScore = "hami.io/node.nvidia.device-pair-score"
const NvidiaGPUDevice      = "NVIDIA"
const MigMode              = "mig"
const HamiCoreMode         = "hami-core"
const MpsMode              = "mps"
```

**`Fit()` å‡½æ•°** â€” æ ¸å¿ƒè®¾å¤‡åˆ†é…ç®—æ³•ï¼ˆL749-885ï¼‰ï¼Œé€ä¸ªæ£€æŸ¥è®¾å¤‡æ˜¯å¦æ»¡è¶³è¯·æ±‚ï¼š

```
æ£€æŸ¥é¡ºåºï¼š
1. è®¾å¤‡å¥åº·çŠ¶æ€ (!dev.Health â†’ skip)
2. ç±»å‹/UUID/NUMA äº²å’Œæ€§è¿‡æ»¤
3. æ—¶é—´ç‰‡é…é¢æ£€æŸ¥ (Count > Used)
4. Quota æ£€æŸ¥ (fitQuota)
5. æ˜¾å­˜æ£€æŸ¥ (Totalmem - Usedmem >= memreq)
6. ç®—åŠ›æ£€æŸ¥ (Totalcore - Usedcores >= Coresreq)
7. ç‹¬å æ¨¡å¼æ£€æŸ¥ (Coresreq=100 â†’ æ’ä»–)
8. MIG CustomFilterRule
9. æ‹“æ‰‘æ„ŸçŸ¥ (topology-aware) æœ€ä¼˜ç»„åˆé€‰æ‹©
```

**MIGï¼ˆMulti-Instance GPUï¼‰æ”¯æŒ** â€” `AddResourceUsage()` (L677-726)ï¼š
- è‡ªåŠ¨é€‰æ‹© MIG Profileï¼ˆvir02/vir04/vir08/vir16ï¼‰
- UUID è¿½åŠ  `[templateIdx-instanceIdx]` æ ¼å¼æ ‡è®° MIG å®ä¾‹

**æ‹“æ‰‘æ„ŸçŸ¥è°ƒåº¦** â€” `computeBestCombination()` / `computeWorstSingleCard()`ï¼š
- å¤šå¡è¯·æ±‚æ—¶é€‰æ‹© NVLink è¿æ¥åˆ†æ•°æœ€é«˜çš„ç»„åˆ
- å•å¡è¯·æ±‚æ—¶é€‰æ‹©ä¸å…¶ä»–å¡è¿æ¥æœ€å·®çš„ï¼ˆé™ä½å¹²æ‰°ï¼‰

---

### ğŸ”· 3. è°ƒåº¦å™¨æ ¸å¿ƒ

#### `pkg/scheduler/scheduler.go` â€” **Scheduler Extender æ ¸å¿ƒ**

```go
type Scheduler struct {
    *nodeManager           // ç®¡ç†èŠ‚ç‚¹è®¾å¤‡ä¿¡æ¯
    podManager             // è·Ÿè¸ªå·²åˆ†é… Pod
    quotaManager           // ResourceQuota ç®¡ç†
    leaderManager          // HA ä¸»ä»é€‰ä¸¾
    cachedstatus           // Filter æ—¶è¿”å›çš„èŠ‚ç‚¹çŠ¶æ€ç¼“å­˜
    overviewstatus         // ç›‘æ§ç”¨å…¨é‡èŠ‚ç‚¹çŠ¶æ€
    // ...
}
```

**ä¸¤å¤§å…¥å£ï¼š**

| å‡½æ•° | èŒè´£ |
|------|------|
| `Filter()` (L644-716) | ä»å€™é€‰èŠ‚ç‚¹ä¸­é€‰å‡ºæœ€ä¼˜èŠ‚ç‚¹ï¼Œæ‰“ Annotationï¼Œå°†è®¾å¤‡åˆ†é…å†™å…¥ Pod |
| `Bind()` (L584-642) | åŠ  NodeLock â†’ Patch Pod Annotation(allocating) â†’ è°ƒç”¨ k8s Bind API |

**`Filter()` å®Œæ•´æµç¨‹ï¼š**
```
1. è§£æ Pod èµ„æºè¯·æ±‚ Resourcereqs()
2. åˆ é™¤æ—§çš„ PodManager ç¼“å­˜ï¼ˆé˜²æ­¢é‡è°ƒåº¦æ±¡æŸ“ï¼‰
3. getNodesUsage() æ„å»ºæ‰€æœ‰å€™é€‰èŠ‚ç‚¹çš„è®¾å¤‡ç”¨é‡å¿«ç…§
4. calcScore() å¹¶å‘è®¡ç®—æ¯ä¸ªèŠ‚ç‚¹å¾—åˆ† + è®¾å¤‡é€‚é…
5. æŒ‰ç­–ç•¥(binpack/spread)æ’åºï¼Œå–æœ€é«˜åˆ†èŠ‚ç‚¹
6. PatchAnnotations å†™å…¥è®¾å¤‡åˆ†é…ç»“æœ
7. AddPod åˆ° PodManager å†…å­˜ç¼“å­˜
8. PatchPodAnnotations æŒä¹…åŒ–åˆ° k8s
```

---

#### `pkg/scheduler/score.go` â€” è¯„åˆ†å¼•æ“

**`calcScore()`** â€” å¹¶å‘è¯„åˆ†ï¼ˆæ¯ä¸ªèŠ‚ç‚¹ä¸€ä¸ª goroutineï¼‰ï¼š

```
å¯¹æ¯ä¸ªèŠ‚ç‚¹ï¼š
1. ComputeDefaultScore() è®¡ç®—èŠ‚ç‚¹åŸºç¡€åˆ† = Weight*(used/total + core/total + mem/total)
2. SnapshotDevice() å¿«ç…§å½“å‰è®¾å¤‡çŠ¶æ€ï¼ˆç”¨äºå›æ»šï¼‰
3. fitInDevices() å°è¯•åœ¨èŠ‚ç‚¹ä¸Šåˆ†é…è®¾å¤‡ï¼ŒæˆåŠŸåˆ™åŠ å…¥å€™é€‰
4. OverrideScore() åŠ ä¸Šè®¾å¤‡çº§åˆ«é™„åŠ åˆ†ï¼ˆå½“å‰ nvidia è¿”å› 0ï¼‰
```

**`fitInDevices()`** â€” è®¾å¤‡é€‚é…ï¼š
```
1. è®¡ç®—æ¯å—è®¾å¤‡çš„ ComputeScore (GPU çº§åˆ«æ‰“åˆ†)
2. æŒ‰ç­–ç•¥æ’åºè®¾å¤‡åˆ—è¡¨ (binpackï¼šåˆ†é«˜æ’å‰ï¼›spreadï¼šåˆ†ä½æ’å‰)
3. è°ƒç”¨ device.Fit() å°è¯•åˆ†é…
4. æˆåŠŸåè°ƒç”¨ AddResourceUsage() æ›´æ–°å†…å­˜ä¸­çš„è®¾å¤‡ä½¿ç”¨é‡
```

---

#### `pkg/scheduler/nodes.go` â€” èŠ‚ç‚¹çŠ¶æ€ç®¡ç†

```go
type NodeUsage struct {
    Node    *corev1.Node
    Devices policy.DeviceUsageList  // å¸¦æ’åºç­–ç•¥çš„è®¾å¤‡åˆ—è¡¨
}

type nodeManager struct {
    nodes map[string]*device.NodeInfo
    mutex sync.RWMutex
}
```

`ListNodes()` è¿”å›æ·±æ‹·è´ï¼Œé˜²æ­¢å¹¶å‘è°ƒåº¦æ—¶æ•°æ®ç«äº‰ã€‚

---

### ğŸ”· 4. è°ƒåº¦ç­–ç•¥å±‚

#### `pkg/scheduler/policy/gpu_policy.go` â€” GPU çº§åˆ«æ’åº

```
è¯„åˆ†å…¬å¼ï¼ˆWeight=10ï¼‰ï¼š
Score = 10 * ((req+used)/count + (coreReq+usedCore)/totalCore + (memReq+usedMem)/totalMem)
binpack: åˆ†é«˜çš„æ’å‰é¢ï¼ˆä¼˜å…ˆæ‰“æ»¡ï¼‰
spread:  åˆ†ä½çš„æ’å‰é¢ï¼ˆä¼˜å…ˆåˆ†æ•£ï¼‰
```

#### `pkg/scheduler/policy/node_policy.go` â€” èŠ‚ç‚¹çº§åˆ«æ’åº

```
èŠ‚ç‚¹åŸºç¡€åˆ† = 10 * (used/total + usedCore/totalCore + usedMem/totalMem)
binpack: NodeScoreList.Less â†’ åˆ†ä½çš„æ’å‰ï¼ˆæœ€ç»ˆå–æœ€åä¸€ä¸ª = åˆ†æœ€é«˜ â†’ æ‰“æ»¡ï¼‰
spread:  åˆ†é«˜çš„æ’å‰ï¼ˆåˆ†æœ€ä½çš„èŠ‚ç‚¹ = æœ€ç©ºé—² â†’ åˆ†æ•£ï¼‰
```

æ³¨æ„ï¼š`sort.Sort()` åå– `NodeList[len-1]`ï¼Œbinpack å–æœ€é«˜åˆ†ï¼ˆæœ€æ»¡ï¼‰ï¼Œspread å–æœ€ä½åˆ†ï¼ˆæœ€ç©ºï¼‰ã€‚

---

### ğŸ”· 5. MutatingWebhook

#### `pkg/scheduler/webhook.go` â€” Pod å‡†å…¥å˜æ›´

```
å¤„ç†æµç¨‹ï¼š
1. è§£ç  Pod
2. è·³è¿‡å·²æœ‰å…¶ä»–è°ƒåº¦å™¨çš„ Pod
3. éå†å®¹å™¨ï¼Œè°ƒç”¨å„è®¾å¤‡çš„ MutateAdmission()
   - æ³¨å…¥ç¯å¢ƒå˜é‡ï¼ˆCUDA_TASK_PRIORITY, GPU_CORE_UTILIZATION_POLICYï¼‰
   - è¡¥å…¨é»˜è®¤ GPU æ•°é‡ï¼ˆåªæœ‰æ˜¾å­˜/ç®—åŠ›è¯·æ±‚æ—¶ï¼‰
   - è®¾ç½® RuntimeClassName
4. ä¿®æ”¹ Pod.Spec.SchedulerName
5. è¿”å› JSON Patch
```

---

### ğŸ”· 6. é…é¢ç®¡ç†

#### `pkg/device/quota.go` â€” ResourceQuota æ„ŸçŸ¥

```
FitQuota() åœ¨ Fit() ä¸­è¢«è°ƒç”¨ï¼Œæ£€æŸ¥ namespace çº§åˆ«é…é¢ï¼š
- æ˜¾å­˜é…é¢ï¼šlimits.nvidia.com/gpumem
- ç®—åŠ›é…é¢ï¼šlimits.nvidia.com/gpucores
QuotaManager æ˜¯å•ä¾‹ï¼ˆsync.Onceï¼‰ï¼Œçº¿ç¨‹å®‰å…¨
```

---

### ğŸ”· 7. NodeLock æœºåˆ¶

#### `pkg/util/nodelock/nodelock.go` â€” é˜²æ­¢å¹¶å‘åˆ†é…å†²çª

```
é”æ ¼å¼ï¼šAnnotation "hami.io/mutex.lock" = "RFC3339æ—¶é—´,namespace,podName"
ç»†ç²’åº¦ï¼šæ¯ä¸ª node æœ‰ç‹¬ç«‹çš„å†…å­˜ mutex (nodeLockManager)
è¶…æ—¶ï¼š5 åˆ†é’Ÿåè¿‡æœŸè‡ªåŠ¨é‡Šæ”¾ï¼ˆHAMI_NODELOCK_EXPIRE å¯é…ç½®ï¼‰
æ‚¬ç©ºé”æ£€æµ‹ï¼šä¸Šä¸€ä¸ª Pod ä¸å­˜åœ¨æ—¶è‡ªåŠ¨é‡Šæ”¾
```

**ç”¨é€”ï¼š** `Bind()` å‰é”å®šèŠ‚ç‚¹ï¼Œé˜²æ­¢å¤šä¸ªè°ƒåº¦å™¨åŒæ—¶å‘åŒä¸€èŠ‚ç‚¹ç»‘å®šå¤šä¸ª GPU ä»»åŠ¡ã€‚

---

### ğŸ”· 8. HA ä¸»ä»é€‰ä¸¾

#### `pkg/util/leaderelection/leaderelection.go` â€” åŸºäº Lease çš„ä¸»ä»

```
ç›‘å¬ Coordination/v1/Lease èµ„æºå˜åŒ–
åˆ¤æ–­è‡ªå·±æ˜¯å¦ Leaderï¼šHolderIdentity.HasPrefix(hostname)
ä¸” Lease æœªè¿‡æœŸ (observedTime + LeaseDuration > now)
OnStartedLeading: é€šçŸ¥ scheduler å¼€å§‹æ³¨å†ŒèŠ‚ç‚¹
OnStoppedLeading: å°† synced ç½® falseï¼Œåœæ­¢æœåŠ¡è°ƒåº¦è¯·æ±‚
```

---

### ğŸ”· 9. vGPU ç›‘æ§ä¸åé¦ˆ

#### `cmd/vGPUmonitor/feedback.go` â€” ä¼˜å…ˆçº§è°ƒåº¦åé¦ˆ

```
é€šè¿‡å…±äº«å†…å­˜ï¼ˆmmapï¼‰è¯»å†™å®¹å™¨å†… libvgpu.so çš„æ§åˆ¶å—ï¼š
- GetRecentKernel() / SetRecentKernel()  â†’ å†…æ ¸æ´»è·ƒåº¦ï¼ˆè´Ÿæ•°=è¢«é˜»å¡ï¼‰
- GetUtilizationSwitch() / SetUtilizationSwitch() â†’ ç®—åŠ›é™åˆ¶å¼€å…³
- GetPriority() â†’ ä»»åŠ¡ä¼˜å…ˆçº§ï¼ˆ0-1ï¼‰

Observe() æ¯ 5 ç§’ï¼š
1. ç»Ÿè®¡æ¯å— GPU ä¸Šå„ä¼˜å…ˆçº§çš„æ´»è·ƒä»»åŠ¡æ•°
2. é«˜ä¼˜å…ˆçº§ä»»åŠ¡å­˜åœ¨æ—¶ï¼Œä½ä¼˜å…ˆçº§ä»»åŠ¡çš„ UtilizationSwitch ç½® 1ï¼ˆè§¦å‘é™æµï¼‰
3. æ— æ³•è°ƒåº¦æ—¶å°† RecentKernel ç½® -1ï¼ˆè§¦å‘é˜»å¡ï¼‰
```

#### `pkg/monitor/nvidia/cudevshr.go` â€” å…±äº«å†…å­˜æ˜ å°„

```
é€šè¿‡ syscall.Mmap + unsafe.Pointer ç›´æ¥è¯»å†™å®¹å™¨çš„ .cache æ–‡ä»¶
æ”¯æŒ v0ï¼ˆå›ºå®šå¤§å° 1197897 å­—èŠ‚ï¼‰å’Œ v1ï¼ˆmajorVersion=1ï¼‰ä¸¤ç§æ ¼å¼
magic flag = 19920718 éªŒè¯æ–‡ä»¶æœ‰æ•ˆæ€§
ç›‘æ§è·¯å¾„ï¼š$HOOK_PATH/containers/{podUID}_{containerName}/
```

---

### ğŸ”· 10. Device Plugin å±‚

#### `cmd/device-plugin/nvidia/main.go` â€” Kubelet Device Plugin

```
å¯åŠ¨æµç¨‹ï¼š
1. inotify ç›‘å¬ /var/lib/kubelet/device-plugins/ (kubelet socket)
2. kubelet é‡å¯æ—¶ï¼ˆsocket é‡å»ºï¼‰è‡ªåŠ¨é‡å¯æ‰€æœ‰æ’ä»¶
3. SIGHUP â†’ é‡å¯ï¼›SIGTERM â†’ ä¼˜é›…é€€å‡º
4. é€šè¿‡ NVML åº“å‘ç° GPU è®¾å¤‡
5. æ”¯æŒ CDIï¼ˆContainer Device Interfaceï¼‰ç°ä»£åŒ–è®¾å¤‡æ³¨å…¥
6. æ”¯æŒ MIG(none/single/mixed)ã€MPSã€æ—¶é—´ç‰‡å…±äº«
```

#### `pkg/device-plugin/nvidiadevice/nvinternal/plugin/` â€” gRPC å®ç°

å®ç° `kubeletdevicepluginv1beta1` æ¥å£ï¼ˆ`ListAndWatch`ã€`Allocate`ã€`GetPreferredAllocation`ï¼‰ã€‚

---

### ğŸ”· 11. å…¨å±€é…ç½®æ³¨å†Œ

#### `pkg/scheduler/config/config.go` â€” è®¾å¤‡æ³¨å†Œæ€»çº¿

```
InitDevicesWithConfig() ç»Ÿä¸€åˆå§‹åŒ–æ‰€æœ‰å¼‚æ„è®¾å¤‡ï¼š
NVIDIA / Cambricon / HYGON / Iluvatar / MThreads /
MetaX / Kunlun / AWSNeuron / AMD / Ascend (HUAWEI)

æ¯ä¸ªè®¾å¤‡æ³¨å†Œåˆ°å…¨å±€ device.DevicesMap[commonWord]
é»˜è®¤é…ç½®å†…åµŒåœ¨ä»£ç ä¸­ï¼ˆInitDefaultDevicesï¼‰ï¼Œä¹Ÿå¯é€šè¿‡ YAML æ–‡ä»¶è¦†ç›–
```

---

## ä¸‰ã€å…³é”®æŠ€æœ¯ç»†èŠ‚æ€»ç»“

### ğŸ”‘ T1. Annotation-Driven çŠ¶æ€ä¼ é€’

æ‰€æœ‰è°ƒåº¦çŠ¶æ€é€šè¿‡ Pod/Node Annotation ä¼ é€’ï¼Œ**æ²¡æœ‰ CRD**ï¼Œè¿™æ˜¯æ ¸å¿ƒè®¾è®¡å†³ç­–ï¼š

| Annotation | ä½ç½® | å«ä¹‰ |
|---|---|---|
| `hami.io/vgpu-devices-to-allocate` | Pod | å¾…åˆ†é…è®¾å¤‡åˆ—è¡¨ |
| `hami.io/vgpu-devices-allocated` | Pod | å·²åˆ†é…è®¾å¤‡åˆ—è¡¨ |
| `hami.io/vgpu-node` | Pod | ç›®æ ‡èŠ‚ç‚¹å |
| `hami.io/bind-phase` | Pod | allocating/failed/success |
| `hami.io/node.nvidia.device-register` | Node | GPU è®¾å¤‡åˆ—è¡¨ï¼ˆJSONï¼‰ |
| `hami.io/node.nvidia.registry.time` | Node | æ¡æ‰‹æ—¶é—´æˆ³ |
| `hami.io/mutex.lock` | Node | èŠ‚ç‚¹å¹¶å‘é” |

### ğŸ”‘ T2. åŒå±‚è¯„åˆ†ä½“ç³»

```
èŠ‚ç‚¹å±‚(NodeScore):
  Score = 10 * (used/total + usedCore/totalCore + usedMem/totalMem)
  binpack â†’ ä¼˜å…ˆé€‰é«˜åˆ†ï¼ˆæœ€æ»¡çš„èŠ‚ç‚¹ï¼‰
  spread  â†’ ä¼˜å…ˆé€‰ä½åˆ†ï¼ˆæœ€ç©ºçš„èŠ‚ç‚¹ï¼‰

GPU å±‚(DeviceListsScore):
  Score = 10 * ((reqCount+used)/count + (reqCore+usedCore)/totalCore + (reqMem+usedMem)/totalMem)
  binpack â†’ ä¼˜å…ˆåˆ†é…é«˜åˆ† GPUï¼ˆæ‰“æ»¡å•å¡ï¼‰
  spread  â†’ ä¼˜å…ˆåˆ†é…ä½åˆ† GPUï¼ˆåˆ†æ•£ä½¿ç”¨ï¼‰
```

### ğŸ”‘ T3. å…±äº«å†…å­˜éš”ç¦»æœºåˆ¶

```
libvgpu.so (é€šè¿‡ ld.so.preload æ³¨å…¥å®¹å™¨)
  â†• mmap å…±äº«å†…å­˜æ–‡ä»¶ (.cache)
vGPUmonitor (å®¿ä¸»æœº DaemonSet)
  - è¯»å–ï¼šGPU ä½¿ç”¨é‡ï¼ˆæ˜¾å­˜ã€SM åˆ©ç”¨ç‡ï¼‰
  - å†™å…¥ï¼šUtilizationSwitchï¼ˆé™é€Ÿå¼€å…³ï¼‰ã€RecentKernelï¼ˆé˜»å¡ä¿¡å·ï¼‰
```

è¿™æ˜¯**æ— ä¾µå…¥å¼**éš”ç¦»çš„å…³é”®ï¼šåº”ç”¨ç¨‹åºä¸éœ€è¦ä¿®æ”¹ï¼Œé€šè¿‡ `LD_PRELOAD` åŠ«æŒ CUDA è°ƒç”¨ã€‚

### ğŸ”‘ T4. è®¾å¤‡å¥åº·æ£€æŸ¥æ¡æ‰‹åè®®

```
device-plugin å¯åŠ¨ â†’ å‘ Node Annotation å†™ "Requesting_<æ—¶é—´æˆ³>"
scheduler æ£€æµ‹åˆ° "Requesting_" â†’ 60 ç§’å†…è®¤ä¸ºå¥åº·ï¼Œç­‰å¾…ä¸ŠæŠ¥
device-plugin å®Œæˆæ³¨å†Œ â†’ å†™å…¥è®¾å¤‡åˆ—è¡¨ JSON
scheduler æ£€æµ‹åˆ°å˜åŒ– â†’ æ›´æ–° nodeManager å†…éƒ¨ç¼“å­˜
device-plugin åœæ­¢ â†’ å†™ "Deleted_<æ—¶é—´æˆ³>"
scheduler æ£€æµ‹åˆ° "Deleted_" â†’ æ ‡è®° needUpdate=false
```

### ğŸ”‘ T5. å¹¶å‘è°ƒåº¦å®‰å…¨

```
ä¸‰å±‚å¹¶å‘ä¿æŠ¤ï¼š
1. nodeManager.mutex (RWMutex) - ä¿æŠ¤èŠ‚ç‚¹ç¼“å­˜è¯»å†™
2. PodManager.mutex (RWMutex) - ä¿æŠ¤ Pod ç¼“å­˜
3. nodelock (per-node Mutex + k8s Annotation) - è·¨å®ä¾‹åˆ†å¸ƒå¼é”

calcScore() ä¸­æ¯ä¸ªèŠ‚ç‚¹ç”¨ç‹¬ç«‹ goroutine å¹¶å‘è®¡ç®—
fitInDevices() æ“ä½œçš„æ˜¯ node çš„æœ¬åœ°æ‹·è´ï¼ˆSnapshotDevice å¿«ç…§ï¼‰
æœ€ç»ˆåªæœ‰é€‰ä¸­çš„èŠ‚ç‚¹çš„ Score è¢«å®é™…å†™å…¥ Pod Annotation
```

### ğŸ”‘ T6. MIGï¼ˆMulti-Instance GPUï¼‰åŠ¨æ€åˆ†é…

```
MIG UUID æ ¼å¼ï¼š{ç‰©ç†GPU_UUID}[{templateIdx}-{instanceIdx}]
ä¾‹å¦‚ï¼šGPU-abc123[0-2] è¡¨ç¤ºç¬¬0å·æ¨¡æ¿çš„ç¬¬2ä¸ªå®ä¾‹

PlatternMIG()       - å°† MIG æ¨¡æ¿å±•å¼€ä¸ºä½¿ç”¨åˆ—è¡¨
migNeedsReset()     - æ£€æµ‹æ˜¯å¦éœ€è¦é‡ç½® MIG ä½¿ç”¨åˆ—è¡¨
AddResourceUsage()  - åˆ†é…æ—¶æ›´æ–° MIG å®ä¾‹çš„ InUse çŠ¶æ€
CustomFilterRule()  - è°ƒåº¦æ—¶æ£€æŸ¥ MIG æ¨¡æ¿æ˜¯å¦æœ‰ç©ºé—²å®ä¾‹
```

### ğŸ”‘ T7. æ‹“æ‰‘æ„ŸçŸ¥è°ƒåº¦ï¼ˆTopology-Awareï¼‰

```
Node Annotation "hami.io/node.nvidia.device-pair-score" å­˜å‚¨ NVLink çŸ©é˜µï¼š
[{"uuid":"GPU-A","score":{"GPU-B":120,"GPU-C":80}}]

å•å¡è¯·æ±‚ â†’ computeWorstSingleCard()ï¼šé€‰ä¸å…¶ä»–å¡è¿æ¥æœ€å¼±çš„ï¼ˆå‡å°‘äº‰ç”¨ï¼‰
å¤šå¡è¯·æ±‚ â†’ computeBestCombination()ï¼šéå†æ‰€æœ‰ç»„åˆï¼Œé€‰ NVLink å¾—åˆ†æ€»å’Œæœ€é«˜çš„
```

---

## å››ã€éœ€è¦é‡ç‚¹å­¦ä¹ çš„æŠ€æœ¯æ ˆ

| é¢†åŸŸ | æŠ€æœ¯ç‚¹ | å¯¹åº”æ–‡ä»¶ |
|------|--------|---------|
| **Kubernetes æ‰©å±•æœºåˆ¶** | Scheduler Extenderã€MutatingWebhook | `pkg/scheduler/` |
| **Informer/Lister æ¨¡å¼** | SharedInformerFactoryã€ResourceEventHandler | `pkg/scheduler/scheduler.go` |
| **Client-go é«˜çº§ç”¨æ³•** | MergePatchã€Retryã€WaitForCacheSync | `pkg/util/util.go`, `nodelock.go` |
| **Device Plugin gRPC** | kubeletdevicepluginv1beta1 åè®® | `pkg/device-plugin/nvinternal/plugin/` |
| **NVML åº“** | GPU ä¿¡æ¯æŸ¥è¯¢ (go-nvml) | `cmd/device-plugin/nvidia/` |
| **CDI è§„èŒƒ** | Container Device Interface | `pkg/device-plugin/nvinternal/cdi/` |
| **mmap å…±äº«å†…å­˜** | syscall.Mmap + unsafe.Pointer | `pkg/monitor/nvidia/cudevshr.go` |
| **LD_PRELOAD æ³¨å…¥** | åŠ¨æ€åº“åŠ«æŒ CUDA è°ƒç”¨ | `lib/nvidia/ld.so.preload` |
| **Prometheus ç›‘æ§** | è‡ªå®šä¹‰ Collectorã€Registry | `cmd/vGPUmonitor/metrics.go` |
| **Leader Election** | Coordination/v1 Lease | `pkg/util/leaderelection/` |
| **æ¥å£æŠ½è±¡è®¾è®¡** | å¤šå¼‚æ„è®¾å¤‡ç»Ÿä¸€æ¥å£ | `pkg/device/devices.go` |
| **å¹¶å‘è°ƒåº¦å®‰å…¨** | åˆ†å¸ƒå¼é” + å†…å­˜é”ç»„åˆ | `pkg/util/nodelock/nodelock.go` |