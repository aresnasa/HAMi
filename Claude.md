# HAMi æ ¸å¿ƒä»£ç åˆ—è¡¨ä¸æŠ€æœ¯ç»†èŠ‚è¯¦è§£

## é¡¹ç›®æ¦‚è¿°

HAMi (Heterogeneous AI Computing Virtualization Middleware) æ˜¯ CNCF æ²™ç›’é¡¹ç›®ï¼Œæ˜¯ä¸€ä¸ª **Kubernetes å¼‚æ„è®¾å¤‡ï¼ˆGPU/NPU/etcï¼‰è™šæ‹ŸåŒ–ä¸­é—´ä»¶**ã€‚å®ƒé€šè¿‡ä¸‰å¤§ç»„ä»¶å®ç° GPU ç­‰è®¾å¤‡çš„å…±äº«ã€éš”ç¦»ä¸è°ƒåº¦ã€‚

---

## ä¸€ã€ç³»ç»Ÿæ¶æ„æ€»è§ˆ

```
[Pod æäº¤]
    |
    v
[MutatingWebhook] â† pkg/scheduler/webhook.go
    | æ³¨å…¥è°ƒåº¦å™¨åç§°ã€ä¿®æ”¹èµ„æºè¯·æ±‚
    v
[Scheduler Extender] â† pkg/scheduler/scheduler.go
    | Filter (é€‰èŠ‚ç‚¹) + Bind (ç»‘å®šèŠ‚ç‚¹)
    v
[Device Plugin] â† cmd/device-plugin/nvidia/
    | å‘ Kubelet æ³¨å†Œè®¾å¤‡èµ„æºã€å“åº” Allocate è¯·æ±‚
    v
[vGPU Monitor] â† cmd/vGPUmonitor/
    | é€šè¿‡ mmap å…±äº«å†…å­˜ç›‘æ§å®¹å™¨ GPU ç”¨é‡ï¼Œå®ç°ä¼˜å…ˆçº§ä¸é™æµåé¦ˆ
    v
[Container] â† lib/nvidia/ld.so.preload â†’ libvgpu.so (C åº“)
    å†…æ ¸çº§æ˜¾å­˜/ç®—åŠ›éš”ç¦»
```

---

## äºŒã€æ ¸å¿ƒä»£ç æ–‡ä»¶æ¸…å•

### ğŸ”· 1. è®¾å¤‡æŠ½è±¡å±‚ï¼ˆDevice Interfaceï¼‰

#### `pkg/device/devices.go` â€” **æœ€æ ¸å¿ƒçš„æ¥å£å®šä¹‰æ–‡ä»¶**

è¿™æ˜¯æ•´ä¸ªå¼‚æ„è®¾å¤‡æŠ½è±¡çš„åŸºçŸ³ï¼š

```go
type Devices interface {
    CommonWord() string
    MutateAdmission(ctr *corev1.Container, pod *corev1.Pod) (bool, error)
    CheckHealth(devType string, n *corev1.Node) (bool, bool)
    NodeCleanUp(nn string) error
    GetResourceNames() ResourceNames
    GetNodeDevices(n corev1.Node) ([]*DeviceInfo, error)
    LockNode(n *corev1.Node, p *corev1.Pod) error
    ReleaseNodeLock(n *corev1.Node, p *corev1.Pod) error
    GenerateResourceRequests(ctr *corev1.Container) ContainerDeviceRequest
    PatchAnnotations(pod *corev1.Pod, annoinput *map[string]string, pd PodDevices) map[string]string
    ScoreNode(node *corev1.Node, podDevices PodSingleDevice, previous []*DeviceUsage, policy string) float32
    AddResourceUsage(pod *corev1.Pod, n *DeviceUsage, ctr *ContainerDevice) error
    Fit(devices []*DeviceUsage, request ContainerDeviceRequest, pod *corev1.Pod, nodeInfo *NodeInfo, allocated *PodDevices) (bool, map[string]ContainerDevices, string)
}
```

**å…³é”®æ•°æ®ç»“æ„ï¼š**

| ç±»å‹ | è¯´æ˜ |
|------|------|
| `DeviceInfo` | èŠ‚ç‚¹ä¸Šå•å—è®¾å¤‡çš„é™æ€ä¿¡æ¯ï¼ˆUUIDã€æ€»æ˜¾å­˜ã€æ ¸æ•°ã€NUMA äº²å’Œæ€§ï¼‰ |
| `DeviceUsage` | è°ƒåº¦æ—¶çš„è®¾å¤‡å®æ—¶ä½¿ç”¨çŠ¶æ€ï¼ˆå·²ç”¨æ˜¾å­˜ã€æ ¸æ•°ã€å ç”¨çš„ Podï¼‰ |
| `ContainerDevice` | åˆ†é…ç»™æŸä¸ªå®¹å™¨çš„ä¸€å—è®¾å¤‡ï¼ˆUUID + ä½¿ç”¨é‡ï¼‰ |
| `PodDevices` | `map[devType]PodSingleDevice`ï¼Œä¸€ä¸ª Pod çš„å…¨éƒ¨è®¾å¤‡åˆ†é…ç»“æœ |
| `ContainerDeviceRequest` | å®¹å™¨çš„èµ„æºè¯·æ±‚ï¼ˆæ•°é‡ Numsã€æ˜¾å­˜ Memreqã€ç®—åŠ› Coresreqï¼‰ |

---

#### `pkg/device/devices.go` â€” Annotation ç¼–è§£ç ç³»åˆ—å‡½æ•°

è®¾å¤‡åˆ†é…ä¿¡æ¯é€šè¿‡ **Kubernetes Pod/Node Annotations** ä¼ é€’ï¼Œç¼–ç åè®®æ˜¯è‡ªå®šä¹‰çš„åˆ†éš”ç¬¦æ ¼å¼ï¼š

```go
// èŠ‚ç‚¹è®¾å¤‡ç¼–ç ï¼šUUID,Count,Mem,Core,Type,Numa,Health,Index,Mode:UUID,...
func EncodeNodeDevices(dlist []*DeviceInfo) string

// Pod å®¹å™¨è®¾å¤‡ç¼–ç ï¼šUUID,Type,UsedMem,UsedCores:UUID,...
func EncodeContainerDevices(cd ContainerDevices) string

// å®¹å™¨é—´ç”¨ ; åˆ†éš”ï¼Œè®¾å¤‡é—´ç”¨ : åˆ†éš”
const OneContainerMultiDeviceSplitSymbol = ":"
const OnePodMultiContainerSplitSymbol   = ";"
```

---

### ğŸ”· 2. NVIDIA GPU è®¾å¤‡å®ç°

#### `pkg/device/nvidia/device.go` â€” **æœ€å¤æ‚çš„è®¾å¤‡å®ç°**

**å…³é”®å¸¸é‡ï¼ˆAnnotation Keysï¼‰ï¼š**
```go
const HandshakeAnnos       = "hami.io/node.nvidia.registry.time"
const RegisterAnnos        = "hami.io/node.nvidia.device-register"
const RegisterGPUPairScore = "hami.io/node.nvidia.device-pair-score"
const NvidiaGPUDevice      = "NVIDIA"
const MigMode              = "mig"
const HamiCoreMode         = "hami-core"
const MpsMode              = "mps"
```

**`Fit()` å‡½æ•°** â€” æ ¸å¿ƒè®¾å¤‡åˆ†é…ç®—æ³•ï¼ˆL749-885ï¼‰ï¼Œé€ä¸ªæ£€æŸ¥è®¾å¤‡æ˜¯å¦æ»¡è¶³è¯·æ±‚ï¼š

```
æ£€æŸ¥é¡ºåºï¼š
1. è®¾å¤‡å¥åº·çŠ¶æ€ (!dev.Health â†’ skip)
2. ç±»å‹/UUID/NUMA äº²å’Œæ€§è¿‡æ»¤
3. æ—¶é—´ç‰‡é…é¢æ£€æŸ¥ (Count > Used)
4. Quota æ£€æŸ¥ (fitQuota)
5. æ˜¾å­˜æ£€æŸ¥ (Totalmem - Usedmem >= memreq)
6. ç®—åŠ›æ£€æŸ¥ (Totalcore - Usedcores >= Coresreq)
7. ç‹¬å æ¨¡å¼æ£€æŸ¥ (Coresreq=100 â†’ æ’ä»–)
8. MIG CustomFilterRule
9. æ‹“æ‰‘æ„ŸçŸ¥ (topology-aware) æœ€ä¼˜ç»„åˆé€‰æ‹©
```

**MIGï¼ˆMulti-Instance GPUï¼‰æ”¯æŒ** â€” `AddResourceUsage()` (L677-726)ï¼š
- è‡ªåŠ¨é€‰æ‹© MIG Profileï¼ˆvir02/vir04/vir08/vir16ï¼‰
- UUID è¿½åŠ  `[templateIdx-instanceIdx]` æ ¼å¼æ ‡è®° MIG å®ä¾‹

**æ‹“æ‰‘æ„ŸçŸ¥è°ƒåº¦** â€” `computeBestCombination()` / `computeWorstSingleCard()`ï¼š
- å¤šå¡è¯·æ±‚æ—¶é€‰æ‹© NVLink è¿æ¥åˆ†æ•°æœ€é«˜çš„ç»„åˆ
- å•å¡è¯·æ±‚æ—¶é€‰æ‹©ä¸å…¶ä»–å¡è¿æ¥æœ€å·®çš„ï¼ˆé™ä½å¹²æ‰°ï¼‰

---

### ğŸ”· 3. è°ƒåº¦å™¨æ ¸å¿ƒ

#### `pkg/scheduler/scheduler.go` â€” **Scheduler Extender æ ¸å¿ƒ**

```go
type Scheduler struct {
    *nodeManager           // ç®¡ç†èŠ‚ç‚¹è®¾å¤‡ä¿¡æ¯
    podManager             // è·Ÿè¸ªå·²åˆ†é… Pod
    quotaManager           // ResourceQuota ç®¡ç†
    leaderManager          // HA ä¸»ä»é€‰ä¸¾
    cachedstatus           // Filter æ—¶è¿”å›çš„èŠ‚ç‚¹çŠ¶æ€ç¼“å­˜
    overviewstatus         // ç›‘æ§ç”¨å…¨é‡èŠ‚ç‚¹çŠ¶æ€
    // ...
}
```

**ä¸¤å¤§å…¥å£ï¼š**

| å‡½æ•° | èŒè´£ |
|------|------|
| `Filter()` (L644-716) | ä»å€™é€‰èŠ‚ç‚¹ä¸­é€‰å‡ºæœ€ä¼˜èŠ‚ç‚¹ï¼Œæ‰“ Annotationï¼Œå°†è®¾å¤‡åˆ†é…å†™å…¥ Pod |
| `Bind()` (L584-642) | åŠ  NodeLock â†’ Patch Pod Annotation(allocating) â†’ è°ƒç”¨ k8s Bind API |

**`Filter()` å®Œæ•´æµç¨‹ï¼š**
```
1. è§£æ Pod èµ„æºè¯·æ±‚ Resourcereqs()
2. åˆ é™¤æ—§çš„ PodManager ç¼“å­˜ï¼ˆé˜²æ­¢é‡è°ƒåº¦æ±¡æŸ“ï¼‰
3. getNodesUsage() æ„å»ºæ‰€æœ‰å€™é€‰èŠ‚ç‚¹çš„è®¾å¤‡ç”¨é‡å¿«ç…§
4. calcScore() å¹¶å‘è®¡ç®—æ¯ä¸ªèŠ‚ç‚¹å¾—åˆ† + è®¾å¤‡é€‚é…
5. æŒ‰ç­–ç•¥(binpack/spread)æ’åºï¼Œå–æœ€é«˜åˆ†èŠ‚ç‚¹
6. PatchAnnotations å†™å…¥è®¾å¤‡åˆ†é…ç»“æœ
7. AddPod åˆ° PodManager å†…å­˜ç¼“å­˜
8. PatchPodAnnotations æŒä¹…åŒ–åˆ° k8s
```

---

#### `pkg/scheduler/score.go` â€” è¯„åˆ†å¼•æ“

**`calcScore()`** â€” å¹¶å‘è¯„åˆ†ï¼ˆæ¯ä¸ªèŠ‚ç‚¹ä¸€ä¸ª goroutineï¼‰ï¼š

```
å¯¹æ¯ä¸ªèŠ‚ç‚¹ï¼š
1. ComputeDefaultScore() è®¡ç®—èŠ‚ç‚¹åŸºç¡€åˆ† = Weight*(used/total + core/total + mem/total)
2. SnapshotDevice() å¿«ç…§å½“å‰è®¾å¤‡çŠ¶æ€ï¼ˆç”¨äºå›æ»šï¼‰
3. fitInDevices() å°è¯•åœ¨èŠ‚ç‚¹ä¸Šåˆ†é…è®¾å¤‡ï¼ŒæˆåŠŸåˆ™åŠ å…¥å€™é€‰
4. OverrideScore() åŠ ä¸Šè®¾å¤‡çº§åˆ«é™„åŠ åˆ†ï¼ˆå½“å‰ nvidia è¿”å› 0ï¼‰
```

**`fitInDevices()`** â€” è®¾å¤‡é€‚é…ï¼š
```
1. è®¡ç®—æ¯å—è®¾å¤‡çš„ ComputeScore (GPU çº§åˆ«æ‰“åˆ†)
2. æŒ‰ç­–ç•¥æ’åºè®¾å¤‡åˆ—è¡¨ (binpackï¼šåˆ†é«˜æ’å‰ï¼›spreadï¼šåˆ†ä½æ’å‰)
3. è°ƒç”¨ device.Fit() å°è¯•åˆ†é…
4. æˆåŠŸåè°ƒç”¨ AddResourceUsage() æ›´æ–°å†…å­˜ä¸­çš„è®¾å¤‡ä½¿ç”¨é‡
```

---

#### `pkg/scheduler/nodes.go` â€” èŠ‚ç‚¹çŠ¶æ€ç®¡ç†

```go
type NodeUsage struct {
    Node    *corev1.Node
    Devices policy.DeviceUsageList  // å¸¦æ’åºç­–ç•¥çš„è®¾å¤‡åˆ—è¡¨
}

type nodeManager struct {
    nodes map[string]*device.NodeInfo
    mutex sync.RWMutex
}
```

`ListNodes()` è¿”å›æ·±æ‹·è´ï¼Œé˜²æ­¢å¹¶å‘è°ƒåº¦æ—¶æ•°æ®ç«äº‰ã€‚

---

### ğŸ”· 4. è°ƒåº¦ç­–ç•¥å±‚

#### `pkg/scheduler/policy/gpu_policy.go` â€” GPU çº§åˆ«æ’åº

```
è¯„åˆ†å…¬å¼ï¼ˆWeight=10ï¼‰ï¼š
Score = 10 * ((req+used)/count + (coreReq+usedCore)/totalCore + (memReq+usedMem)/totalMem)
binpack: åˆ†é«˜çš„æ’å‰é¢ï¼ˆä¼˜å…ˆæ‰“æ»¡ï¼‰
spread:  åˆ†ä½çš„æ’å‰é¢ï¼ˆä¼˜å…ˆåˆ†æ•£ï¼‰
```

#### `pkg/scheduler/policy/node_policy.go` â€” èŠ‚ç‚¹çº§åˆ«æ’åº

```
èŠ‚ç‚¹åŸºç¡€åˆ† = 10 * (used/total + usedCore/totalCore + usedMem/totalMem)
binpack: NodeScoreList.Less â†’ åˆ†ä½çš„æ’å‰ï¼ˆæœ€ç»ˆå–æœ€åä¸€ä¸ª = åˆ†æœ€é«˜ â†’ æ‰“æ»¡ï¼‰
spread:  åˆ†é«˜çš„æ’å‰ï¼ˆåˆ†æœ€ä½çš„èŠ‚ç‚¹ = æœ€ç©ºé—² â†’ åˆ†æ•£ï¼‰
```

æ³¨æ„ï¼š`sort.Sort()` åå– `NodeList[len-1]`ï¼Œbinpack å–æœ€é«˜åˆ†ï¼ˆæœ€æ»¡ï¼‰ï¼Œspread å–æœ€ä½åˆ†ï¼ˆæœ€ç©ºï¼‰ã€‚

---

### ğŸ”· 5. MutatingWebhook

#### `pkg/scheduler/webhook.go` â€” Pod å‡†å…¥å˜æ›´

```
å¤„ç†æµç¨‹ï¼š
1. è§£ç  Pod
2. è·³è¿‡å·²æœ‰å…¶ä»–è°ƒåº¦å™¨çš„ Pod
3. éå†å®¹å™¨ï¼Œè°ƒç”¨å„è®¾å¤‡çš„ MutateAdmission()
   - æ³¨å…¥ç¯å¢ƒå˜é‡ï¼ˆCUDA_TASK_PRIORITY, GPU_CORE_UTILIZATION_POLICYï¼‰
   - è¡¥å…¨é»˜è®¤ GPU æ•°é‡ï¼ˆåªæœ‰æ˜¾å­˜/ç®—åŠ›è¯·æ±‚æ—¶ï¼‰
   - è®¾ç½® RuntimeClassName
4. ä¿®æ”¹ Pod.Spec.SchedulerName
5. è¿”å› JSON Patch
```

---

### ğŸ”· 6. é…é¢ç®¡ç†

#### `pkg/device/quota.go` â€” ResourceQuota æ„ŸçŸ¥

```
FitQuota() åœ¨ Fit() ä¸­è¢«è°ƒç”¨ï¼Œæ£€æŸ¥ namespace çº§åˆ«é…é¢ï¼š
- æ˜¾å­˜é…é¢ï¼šlimits.nvidia.com/gpumem
- ç®—åŠ›é…é¢ï¼šlimits.nvidia.com/gpucores
QuotaManager æ˜¯å•ä¾‹ï¼ˆsync.Onceï¼‰ï¼Œçº¿ç¨‹å®‰å…¨
```

---

### ğŸ”· 7. NodeLock æœºåˆ¶

#### `pkg/util/nodelock/nodelock.go` â€” é˜²æ­¢å¹¶å‘åˆ†é…å†²çª

```
é”æ ¼å¼ï¼šAnnotation "hami.io/mutex.lock" = "RFC3339æ—¶é—´,namespace,podName"
ç»†ç²’åº¦ï¼šæ¯ä¸ª node æœ‰ç‹¬ç«‹çš„å†…å­˜ mutex (nodeLockManager)
è¶…æ—¶ï¼š5 åˆ†é’Ÿåè¿‡æœŸè‡ªåŠ¨é‡Šæ”¾ï¼ˆHAMI_NODELOCK_EXPIRE å¯é…ç½®ï¼‰
æ‚¬ç©ºé”æ£€æµ‹ï¼šä¸Šä¸€ä¸ª Pod ä¸å­˜åœ¨æ—¶è‡ªåŠ¨é‡Šæ”¾
```

**ç”¨é€”ï¼š** `Bind()` å‰é”å®šèŠ‚ç‚¹ï¼Œé˜²æ­¢å¤šä¸ªè°ƒåº¦å™¨åŒæ—¶å‘åŒä¸€èŠ‚ç‚¹ç»‘å®šå¤šä¸ª GPU ä»»åŠ¡ã€‚

---

### ğŸ”· 8. HA ä¸»ä»é€‰ä¸¾

#### `pkg/util/leaderelection/leaderelection.go` â€” åŸºäº Lease çš„ä¸»ä»

```
ç›‘å¬ Coordination/v1/Lease èµ„æºå˜åŒ–
åˆ¤æ–­è‡ªå·±æ˜¯å¦ Leaderï¼šHolderIdentity.HasPrefix(hostname)
ä¸” Lease æœªè¿‡æœŸ (observedTime + LeaseDuration > now)
OnStartedLeading: é€šçŸ¥ scheduler å¼€å§‹æ³¨å†ŒèŠ‚ç‚¹
OnStoppedLeading: å°† synced ç½® falseï¼Œåœæ­¢æœåŠ¡è°ƒåº¦è¯·æ±‚
```

---

### ğŸ”· 9. vGPU ç›‘æ§ä¸åé¦ˆ

#### `cmd/vGPUmonitor/feedback.go` â€” ä¼˜å…ˆçº§è°ƒåº¦åé¦ˆ

```
é€šè¿‡å…±äº«å†…å­˜ï¼ˆmmapï¼‰è¯»å†™å®¹å™¨å†… libvgpu.so çš„æ§åˆ¶å—ï¼š
- GetRecentKernel() / SetRecentKernel()  â†’ å†…æ ¸æ´»è·ƒåº¦ï¼ˆè´Ÿæ•°=è¢«é˜»å¡ï¼‰
- GetUtilizationSwitch() / SetUtilizationSwitch() â†’ ç®—åŠ›é™åˆ¶å¼€å…³
- GetPriority() â†’ ä»»åŠ¡ä¼˜å…ˆçº§ï¼ˆ0-1ï¼‰

Observe() æ¯ 5 ç§’ï¼š
1. ç»Ÿè®¡æ¯å— GPU ä¸Šå„ä¼˜å…ˆçº§çš„æ´»è·ƒä»»åŠ¡æ•°
2. é«˜ä¼˜å…ˆçº§ä»»åŠ¡å­˜åœ¨æ—¶ï¼Œä½ä¼˜å…ˆçº§ä»»åŠ¡çš„ UtilizationSwitch ç½® 1ï¼ˆè§¦å‘é™æµï¼‰
3. æ— æ³•è°ƒåº¦æ—¶å°† RecentKernel ç½® -1ï¼ˆè§¦å‘é˜»å¡ï¼‰
```

#### `pkg/monitor/nvidia/cudevshr.go` â€” å…±äº«å†…å­˜æ˜ å°„

```
é€šè¿‡ syscall.Mmap + unsafe.Pointer ç›´æ¥è¯»å†™å®¹å™¨çš„ .cache æ–‡ä»¶
æ”¯æŒ v0ï¼ˆå›ºå®šå¤§å° 1197897 å­—èŠ‚ï¼‰å’Œ v1ï¼ˆmajorVersion=1ï¼‰ä¸¤ç§æ ¼å¼
magic flag = 19920718 éªŒè¯æ–‡ä»¶æœ‰æ•ˆæ€§
ç›‘æ§è·¯å¾„ï¼š$HOOK_PATH/containers/{podUID}_{containerName}/
```

---

### ğŸ”· 10. Device Plugin å±‚

#### `cmd/device-plugin/nvidia/main.go` â€” Kubelet Device Plugin

```
å¯åŠ¨æµç¨‹ï¼š
1. inotify ç›‘å¬ /var/lib/kubelet/device-plugins/ (kubelet socket)
2. kubelet é‡å¯æ—¶ï¼ˆsocket é‡å»ºï¼‰è‡ªåŠ¨é‡å¯æ‰€æœ‰æ’ä»¶
3. SIGHUP â†’ é‡å¯ï¼›SIGTERM â†’ ä¼˜é›…é€€å‡º
4. é€šè¿‡ NVML åº“å‘ç° GPU è®¾å¤‡
5. æ”¯æŒ CDIï¼ˆContainer Device Interfaceï¼‰ç°ä»£åŒ–è®¾å¤‡æ³¨å…¥
6. æ”¯æŒ MIG(none/single/mixed)ã€MPSã€æ—¶é—´ç‰‡å…±äº«
```

#### `pkg/device-plugin/nvidiadevice/nvinternal/plugin/` â€” gRPC å®ç°

å®ç° `kubeletdevicepluginv1beta1` æ¥å£ï¼ˆ`ListAndWatch`ã€`Allocate`ã€`GetPreferredAllocation`ï¼‰ã€‚

---

### ğŸ”· 11. å…¨å±€é…ç½®æ³¨å†Œ

#### `pkg/scheduler/config/config.go` â€” è®¾å¤‡æ³¨å†Œæ€»çº¿

```
InitDevicesWithConfig() ç»Ÿä¸€åˆå§‹åŒ–æ‰€æœ‰å¼‚æ„è®¾å¤‡ï¼š
NVIDIA / Cambricon / HYGON / Iluvatar / MThreads /
MetaX / Kunlun / AWSNeuron / AMD / Ascend (HUAWEI)

æ¯ä¸ªè®¾å¤‡æ³¨å†Œåˆ°å…¨å±€ device.DevicesMap[commonWord]
é»˜è®¤é…ç½®å†…åµŒåœ¨ä»£ç ä¸­ï¼ˆInitDefaultDevicesï¼‰ï¼Œä¹Ÿå¯é€šè¿‡ YAML æ–‡ä»¶è¦†ç›–
```

---

## ä¸‰ã€å…³é”®æŠ€æœ¯ç»†èŠ‚æ€»ç»“

### ğŸ”‘ T1. Annotation-Driven çŠ¶æ€ä¼ é€’

æ‰€æœ‰è°ƒåº¦çŠ¶æ€é€šè¿‡ Pod/Node Annotation ä¼ é€’ï¼Œ**æ²¡æœ‰ CRD**ï¼Œè¿™æ˜¯æ ¸å¿ƒè®¾è®¡å†³ç­–ï¼š

| Annotation | ä½ç½® | å«ä¹‰ |
|---|---|---|
| `hami.io/vgpu-devices-to-allocate` | Pod | å¾…åˆ†é…è®¾å¤‡åˆ—è¡¨ |
| `hami.io/vgpu-devices-allocated` | Pod | å·²åˆ†é…è®¾å¤‡åˆ—è¡¨ |
| `hami.io/vgpu-node` | Pod | ç›®æ ‡èŠ‚ç‚¹å |
| `hami.io/bind-phase` | Pod | allocating/failed/success |
| `hami.io/node.nvidia.device-register` | Node | GPU è®¾å¤‡åˆ—è¡¨ï¼ˆJSONï¼‰ |
| `hami.io/node.nvidia.registry.time` | Node | æ¡æ‰‹æ—¶é—´æˆ³ |
| `hami.io/mutex.lock` | Node | èŠ‚ç‚¹å¹¶å‘é” |

### ğŸ”‘ T2. åŒå±‚è¯„åˆ†ä½“ç³»

```
èŠ‚ç‚¹å±‚(NodeScore):
  Score = 10 * (used/total + usedCore/totalCore + usedMem/totalMem)
  binpack â†’ ä¼˜å…ˆé€‰é«˜åˆ†ï¼ˆæœ€æ»¡çš„èŠ‚ç‚¹ï¼‰
  spread  â†’ ä¼˜å…ˆé€‰ä½åˆ†ï¼ˆæœ€ç©ºçš„èŠ‚ç‚¹ï¼‰

GPU å±‚(DeviceListsScore):
  Score = 10 * ((reqCount+used)/count + (reqCore+usedCore)/totalCore + (reqMem+usedMem)/totalMem)
  binpack â†’ ä¼˜å…ˆåˆ†é…é«˜åˆ† GPUï¼ˆæ‰“æ»¡å•å¡ï¼‰
  spread  â†’ ä¼˜å…ˆåˆ†é…ä½åˆ† GPUï¼ˆåˆ†æ•£ä½¿ç”¨ï¼‰
```

### ğŸ”‘ T3. å…±äº«å†…å­˜éš”ç¦»æœºåˆ¶

```
libvgpu.so (é€šè¿‡ ld.so.preload æ³¨å…¥å®¹å™¨)
  â†• mmap å…±äº«å†…å­˜æ–‡ä»¶ (.cache)
vGPUmonitor (å®¿ä¸»æœº DaemonSet)
  - è¯»å–ï¼šGPU ä½¿ç”¨é‡ï¼ˆæ˜¾å­˜ã€SM åˆ©ç”¨ç‡ï¼‰
  - å†™å…¥ï¼šUtilizationSwitchï¼ˆé™é€Ÿå¼€å…³ï¼‰ã€RecentKernelï¼ˆé˜»å¡ä¿¡å·ï¼‰
```

è¿™æ˜¯**æ— ä¾µå…¥å¼**éš”ç¦»çš„å…³é”®ï¼šåº”ç”¨ç¨‹åºä¸éœ€è¦ä¿®æ”¹ï¼Œé€šè¿‡ `LD_PRELOAD` åŠ«æŒ CUDA è°ƒç”¨ã€‚

### ğŸ”‘ T4. è®¾å¤‡å¥åº·æ£€æŸ¥æ¡æ‰‹åè®®

```
device-plugin å¯åŠ¨ â†’ å‘ Node Annotation å†™ "Requesting_<æ—¶é—´æˆ³>"
scheduler æ£€æµ‹åˆ° "Requesting_" â†’ 60 ç§’å†…è®¤ä¸ºå¥åº·ï¼Œç­‰å¾…ä¸ŠæŠ¥
device-plugin å®Œæˆæ³¨å†Œ â†’ å†™å…¥è®¾å¤‡åˆ—è¡¨ JSON
scheduler æ£€æµ‹åˆ°å˜åŒ– â†’ æ›´æ–° nodeManager å†…éƒ¨ç¼“å­˜
device-plugin åœæ­¢ â†’ å†™ "Deleted_<æ—¶é—´æˆ³>"
scheduler æ£€æµ‹åˆ° "Deleted_" â†’ æ ‡è®° needUpdate=false
```

### ğŸ”‘ T5. å¹¶å‘è°ƒåº¦å®‰å…¨

```
ä¸‰å±‚å¹¶å‘ä¿æŠ¤ï¼š
1. nodeManager.mutex (RWMutex) - ä¿æŠ¤èŠ‚ç‚¹ç¼“å­˜è¯»å†™
2. PodManager.mutex (RWMutex) - ä¿æŠ¤ Pod ç¼“å­˜
3. nodelock (per-node Mutex + k8s Annotation) - è·¨å®ä¾‹åˆ†å¸ƒå¼é”

calcScore() ä¸­æ¯ä¸ªèŠ‚ç‚¹ç”¨ç‹¬ç«‹ goroutine å¹¶å‘è®¡ç®—
fitInDevices() æ“ä½œçš„æ˜¯ node çš„æœ¬åœ°æ‹·è´ï¼ˆSnapshotDevice å¿«ç…§ï¼‰
æœ€ç»ˆåªæœ‰é€‰ä¸­çš„èŠ‚ç‚¹çš„ Score è¢«å®é™…å†™å…¥ Pod Annotation
```

### ğŸ”‘ T6. MIGï¼ˆMulti-Instance GPUï¼‰åŠ¨æ€åˆ†é…

```
MIG UUID æ ¼å¼ï¼š{ç‰©ç†GPU_UUID}[{templateIdx}-{instanceIdx}]
ä¾‹å¦‚ï¼šGPU-abc123[0-2] è¡¨ç¤ºç¬¬0å·æ¨¡æ¿çš„ç¬¬2ä¸ªå®ä¾‹

PlatternMIG()       - å°† MIG æ¨¡æ¿å±•å¼€ä¸ºä½¿ç”¨åˆ—è¡¨
migNeedsReset()     - æ£€æµ‹æ˜¯å¦éœ€è¦é‡ç½® MIG ä½¿ç”¨åˆ—è¡¨
AddResourceUsage()  - åˆ†é…æ—¶æ›´æ–° MIG å®ä¾‹çš„ InUse çŠ¶æ€
CustomFilterRule()  - è°ƒåº¦æ—¶æ£€æŸ¥ MIG æ¨¡æ¿æ˜¯å¦æœ‰ç©ºé—²å®ä¾‹
```

### ğŸ”‘ T7. æ‹“æ‰‘æ„ŸçŸ¥è°ƒåº¦ï¼ˆTopology-Awareï¼‰

```
Node Annotation "hami.io/node.nvidia.device-pair-score" å­˜å‚¨ NVLink çŸ©é˜µï¼š
[{"uuid":"GPU-A","score":{"GPU-B":120,"GPU-C":80}}]

å•å¡è¯·æ±‚ â†’ computeWorstSingleCard()ï¼šé€‰ä¸å…¶ä»–å¡è¿æ¥æœ€å¼±çš„ï¼ˆå‡å°‘äº‰ç”¨ï¼‰
å¤šå¡è¯·æ±‚ â†’ computeBestCombination()ï¼šéå†æ‰€æœ‰ç»„åˆï¼Œé€‰ NVLink å¾—åˆ†æ€»å’Œæœ€é«˜çš„
```

---

## å››ã€éœ€è¦é‡ç‚¹å­¦ä¹ çš„æŠ€æœ¯æ ˆ

| é¢†åŸŸ | æŠ€æœ¯ç‚¹ | å¯¹åº”æ–‡ä»¶ |
|------|--------|---------|
| **Kubernetes æ‰©å±•æœºåˆ¶** | Scheduler Extenderã€MutatingWebhook | `pkg/scheduler/` |
| **Informer/Lister æ¨¡å¼** | SharedInformerFactoryã€ResourceEventHandler | `pkg/scheduler/scheduler.go` |
| **Client-go é«˜çº§ç”¨æ³•** | MergePatchã€Retryã€WaitForCacheSync | `pkg/util/util.go`, `nodelock.go` |
| **Device Plugin gRPC** | kubeletdevicepluginv1beta1 åè®® | `pkg/device-plugin/nvinternal/plugin/` |
| **NVML åº“** | GPU ä¿¡æ¯æŸ¥è¯¢ (go-nvml) | `cmd/device-plugin/nvidia/` |
| **CDI è§„èŒƒ** | Container Device Interface | `pkg/device-plugin/nvinternal/cdi/` |
| **mmap å…±äº«å†…å­˜** | syscall.Mmap + unsafe.Pointer | `pkg/monitor/nvidia/cudevshr.go` |
| **LD_PRELOAD æ³¨å…¥** | åŠ¨æ€åº“åŠ«æŒ CUDA è°ƒç”¨ | `lib/nvidia/ld.so.preload` |
| **Prometheus ç›‘æ§** | è‡ªå®šä¹‰ Collectorã€Registry | `cmd/vGPUmonitor/metrics.go` |
| **Leader Election** | Coordination/v1 Lease | `pkg/util/leaderelection/` |
| **æ¥å£æŠ½è±¡è®¾è®¡** | å¤šå¼‚æ„è®¾å¤‡ç»Ÿä¸€æ¥å£ | `pkg/device/devices.go` |
| **å¹¶å‘è°ƒåº¦å®‰å…¨** | åˆ†å¸ƒå¼é” + å†…å­˜é”ç»„åˆ | `pkg/util/nodelock/nodelock.go` |

---

## äº”ã€Upstream åŒæ­¥å˜æ›´è®°å½•ï¼ˆ2026-02 æ‰¹æ¬¡ï¼Œå…± 31 commitsï¼‰

> åŒæ­¥è‡ª `Project-HAMi/HAMi@cb077d5`ï¼Œåˆå¹¶åˆ°æœ¬ fork `master` åˆ†æ”¯ã€‚

### ğŸ› Bug ä¿®å¤

#### BUG-1ï¼š`calcScore()` ä¸­ `ctrfit` åˆå§‹å€¼é”™è¯¯å¯¼è‡´ panicï¼ˆ#1626ï¼‰
- **æ–‡ä»¶**ï¼š`pkg/scheduler/score.go`
- **é—®é¢˜**ï¼š`ctrfit` åˆå§‹å€¼ä¸º `false`ï¼Œå½“ Pod æ²¡æœ‰ä»»ä½•è®¾å¤‡è¯·æ±‚ï¼ˆ`resourceReqs` ä¸º nilï¼‰æ—¶ï¼Œ`range` å¾ªç¯ä¸æ‰§è¡Œï¼ŒèŠ‚ç‚¹è¢«é”™è¯¯æ’é™¤ï¼Œå¹¶å¯èƒ½è§¦å‘ panicï¼ˆissue #1327ï¼‰ã€‚
- **ä¿®å¤**ï¼šå°†åˆå§‹å€¼æ”¹ä¸º `true`ã€‚Go çš„ `range nil` å®‰å…¨è·³è¿‡å¾ªç¯ï¼Œæ— è®¾å¤‡éœ€æ±‚çš„ Pod åº”èƒ½è°ƒåº¦åˆ°ä»»æ„èŠ‚ç‚¹ã€‚
```
// ä¿®å¤å‰
ctrfit := false
// ä¿®å¤å
ctrfit := true  // Pod æ— è®¾å¤‡éœ€æ±‚æ—¶ï¼Œé»˜è®¤èŠ‚ç‚¹é€‚é…
```

#### BUG-2ï¼šWebhook ä¸­è°ƒåº¦å™¨åç§°åˆ¤æ–­è¿ç®—ç¬¦ä¼˜å…ˆçº§é”™è¯¯ï¼ˆ#1627ï¼‰
- **æ–‡ä»¶**ï¼š`pkg/scheduler/webhook.go`
- **é—®é¢˜**ï¼š`||` å’Œ `&&` æ··ç”¨æ—¶ç¼ºå°‘æ‹¬å·ï¼Œå¯¼è‡´ `ForceOverwriteDefaultScheduler=false` æ—¶é€»è¾‘çŸ­è·¯å¼‚å¸¸ï¼Œå…·æœ‰èµ„æºè¯·æ±‚çš„ Pod è¢«é”™è¯¯æ”¾è¡Œè€Œä¸æ³¨å…¥è°ƒåº¦å™¨åç§°ã€‚
- **ä¿®å¤**ï¼šæ·»åŠ æ‹¬å·æ˜ç¡®ä¼˜å…ˆçº§ã€‚
```go
// ä¿®å¤å‰ï¼ˆæœ‰æ­§ä¹‰ï¼‰
if pod.Spec.SchedulerName != "" &&
    pod.Spec.SchedulerName != corev1.DefaultSchedulerName || !config.ForceOverwriteDefaultScheduler &&
    ...
// ä¿®å¤åï¼ˆæ˜ç¡®ï¼‰
if pod.Spec.SchedulerName != "" &&
    (pod.Spec.SchedulerName != corev1.DefaultSchedulerName || !config.ForceOverwriteDefaultScheduler) &&
    ...
```

#### BUG-3ï¼šLeaderElection ä¸­ nil æŒ‡é’ˆ panicï¼ˆ#1603ï¼‰
- **æ–‡ä»¶**ï¼š`pkg/util/leaderelection/leaderelection.go`
- **é—®é¢˜**ï¼š`OnStartedLeading`/`OnStoppedLeading` callback ä¸º nil æ—¶ç›´æ¥è°ƒç”¨ä¼š panicï¼›`isHolderOf()` æœªæ£€æŸ¥ `lease == nil`ï¼›`isLeaseValid()` æœªæ£€æŸ¥ `LeaseDurationSeconds == nil`ã€‚
- **ä¿®å¤**ï¼šæ‰€æœ‰è°ƒç”¨ç‚¹å‰åŠ  nil guardï¼›`isHolderOf()` å’Œ `isLeaseValid()` åŠ é˜²å¾¡æ€§æ£€æŸ¥ã€‚

#### BUG-4ï¼šIluvatar è®¾å¤‡ binpack/spread ç­–ç•¥åè½¬ï¼ˆ#1631ï¼‰
- **æ–‡ä»¶**ï¼š`pkg/device/iluvatar/device.go`
- **é—®é¢˜**ï¼šIluvatar è®¾å¤‡å®ç°ä¸­ binpack å’Œ spread æ’åºé€»è¾‘å†™åï¼Œå¯¼
è‡´ binpack æ—¶å®é™…æ‰§è¡Œ spread è¡Œä¸ºã€‚
- **ä¿®å¤**ï¼šäº¤æ¢æ’åºæ¯”è¾ƒæ–¹å‘ï¼Œä¸ NVIDIA å®ç°ä¿æŒä¸€è‡´ã€‚

#### BUG-5ï¼šç»Ÿä¸€æ˜¾å­˜ GPUï¼ˆGB10/DGX Sparkï¼‰`GetMemoryInfo` è¿”å› NOT_SUPPORTED å¯¼è‡´ panicï¼ˆ#1637ï¼‰
- **æ–‡ä»¶**ï¼š`pkg/device-plugin/nvidiadevice/nvinternal/plugin/register.go`ã€`cmd/vGPUmonitor/metrics.go`ã€`pkg/device/nvidia/device.go`
- **é—®é¢˜**ï¼šNVIDIA GB10 ç­‰ç»Ÿä¸€å†…å­˜æ¶æ„ GPU è°ƒç”¨ `nvmlDeviceGetMemoryInfo()` è¿”å› `ERROR_NOT_SUPPORTED`ï¼ŒåŸä»£ç ç›´æ¥ panicã€‚
- **ä¿®å¤**ï¼š
  - `register.go`ï¼šæ•è· `NOT_SUPPORTED`ï¼Œå›é€€åˆ°é…ç½®æ–‡ä»¶ä¸­çš„ `PreConfiguredDeviceMemory` å€¼ï¼›è‹¥æœªé…ç½®åˆ™è·³è¿‡è¯¥è®¾å¤‡ï¼ˆ`continue`ï¼‰è€Œé panicã€‚
  - `metrics.go`ï¼šè·³è¿‡ä¸æ”¯æŒæ˜¾å­˜æŸ¥è¯¢çš„è®¾å¤‡çš„å†…å­˜æŒ‡æ ‡é‡‡é›†ã€‚
  - `device.go`ï¼š`NodeDefaultConfig` æ–°å¢ `PreConfiguredDeviceMemory` å­—æ®µï¼Œæ”¯æŒ Helm values æŒ‰èŠ‚ç‚¹é…ç½®ã€‚

#### BUG-6ï¼š`Device_memory_desc_of_container` æŒ‡æ ‡åŸºæ•°çˆ†ç‚¸ï¼ˆ#1628ï¼‰
- **æ–‡ä»¶**ï¼š`cmd/vGPUmonitor/metrics.go`
- **é—®é¢˜**ï¼šæ¯ä¸ªå®¹å™¨çš„æ¯å—è®¾å¤‡éƒ½ç”Ÿæˆç‹¬ç«‹ label ç»´åº¦ç»„åˆï¼Œéšå®¹å™¨æ•°é‡çº¿æ€§å¢é•¿ï¼Œå¯¼è‡´ Prometheus åŸºæ•°çˆ†ç‚¸ï¼ˆcardinality explosionï¼‰ï¼Œå†…å­˜å ç”¨æ€¥å‰§ä¸Šå‡ã€‚
- **ä¿®å¤**ï¼šç»Ÿä¸€æ˜¾å­˜å’Œåˆ©ç”¨ç‡æŒ‡æ ‡çš„ label é›†åˆï¼Œåˆå¹¶å†—ä½™ç»´åº¦ï¼Œå‡å°‘æ—¶é—´åºåˆ—æ•°é‡ã€‚

---

### ğŸ”’ å®‰å…¨åŠ å›º

#### SEC-1ï¼šHTTP è¯·æ±‚ä½“å¤§å°é™åˆ¶ï¼Œé˜²æ­¢ DoSï¼ˆ#1620ï¼‰
- **æ–‡ä»¶**ï¼š`pkg/scheduler/routes/route.go`
- **é—®é¢˜**ï¼š`/predicate` å’Œ `/bind` ä¸¤ä¸ª HTTP ç«¯ç‚¹æœªé™åˆ¶è¯·æ±‚ä½“å¤§å°ï¼Œæ”»å‡»è€…å¯å‘é€è¶…å¤§ payload è€—å°½å†…å­˜ï¼ˆissue #554ï¼‰ã€‚
- **ä¿®å¤**ï¼šç”¨ `io.LimitReader` åŒ…è£… `r.Body`ï¼Œé™åˆ¶ä¸º 1MBã€‚
```go
const maxRequestSize = 1024 * 1024 // 1MB
limitedReader := io.LimitReader(r.Body, maxRequestSize)
body := io.TeeReader(limitedReader, &buf)
```

---

### âœ¨ æ–°åŠŸèƒ½

#### FEAT-1ï¼šWebhook é˜¶æ®µæå‰æ£€æŸ¥ ResourceQuotaï¼ˆ#1605ï¼‰
- **æ–‡ä»¶**ï¼š`pkg/scheduler/webhook.go`
- **æ–°å¢**ï¼š`fitResourceQuota()` å‡½æ•°ï¼Œåœ¨ Pod å‡†å…¥é˜¶æ®µï¼ˆWebhookï¼‰å³æ£€æŸ¥ namespace çº§åˆ«çš„æ˜¾å­˜/ç®—åŠ›é…é¢ï¼Œé…é¢ä¸è¶³æ—¶ç›´æ¥ Denyï¼Œé¿å… Pod è¿›å…¥è°ƒåº¦é˜Ÿåˆ—åæ‰å¤±è´¥ã€‚
- **å½“å‰é™åˆ¶**ï¼šä»…æ”¯æŒ NVIDIA GPU è®¾å¤‡ã€‚
- **æµç¨‹ä½ç½®**ï¼šä½äº `MutateAdmission` ä¹‹åã€`json.Marshal` ä¹‹å‰ã€‚
```go
if !fitResourceQuota(pod) {
    return admission.Denied("exceeding resource quota")
}
```

#### FEAT-2ï¼šAscend 910C SuperPod æ¨¡å—å¯¹å„¿åˆ†é…ï¼ˆ#1610ï¼‰
- **æ–‡ä»¶**ï¼š`pkg/device/ascend/device.go`
- **èƒŒæ™¯**ï¼šAscend 910C çš„ç‰©ç†æ¶æ„ä¸­ï¼Œæœ€å°åˆ†é…å•å…ƒæ˜¯ä¸€ä¸ªç‰©ç†æ¨¡å—ï¼ˆ2 ä¸ª NPUï¼‰ã€‚è¯·æ±‚ 1 ä¸ª NPU å®é™…éœ€è¦å ç”¨ 2 ä¸ªã€‚
- **å®ç°**ï¼š
  - `MutateAdmission()` ä¸­æ£€æµ‹ `Ascend910C` è®¾å¤‡ç±»å‹ï¼šè¯·æ±‚æ•°ä¸º 1 æ—¶è‡ªåŠ¨æ‰©å±•ä¸º 2ï¼›å¥‡æ•°è¯·æ±‚ï¼ˆ3ã€5ã€7â€¦ï¼‰ç›´æ¥æ‹’ç»å¹¶è¿”å›é”™è¯¯ã€‚
  - `Fit()` ä¸­æ–°å¢ `computeBestCombination910C()`ï¼šæŒ‰ç‰©ç†å¡ï¼ˆæ¯å¡ 2 NPUï¼‰åˆ†ç»„ï¼Œé€‰æ‹©åŒä¸€ç‰©ç†æ¨¡å—å†…çš„ NPU å¯¹è¿›è¡Œåˆ†é…ï¼Œä¿è¯æ¨¡å—å†…å±€éƒ¨æ€§ã€‚
```go
const Ascend910CType = "Ascend910C"
// MutateAdmission ä¸­
if reqNum == 1 {
    reqNum = 2  // è‡ªåŠ¨æ‰©å±•åˆ°æœ€å°åˆ†é…å•å…ƒ
} else if reqNum%2 != 0 {
    return false, errors.New("Ascend910C device request must be 1 or 2*n")
}
```

#### FEAT-3ï¼šPrometheus ServiceMonitor æ”¯æŒï¼ˆ#1614ã€#1633ï¼‰
- **æ–‡ä»¶**ï¼š`charts/hami/templates/scheduler/servicemonitor.yaml`ã€`charts/hami/templates/device-plugin/servicemonitor.yaml`
- **æ–°å¢**ï¼šHelm chart ä¸­ä¸º scheduler å’Œ device-plugin åˆ†åˆ«æ·»åŠ  `ServiceMonitor` CRD èµ„æºï¼Œé…åˆ Prometheus Operator å®ç°è‡ªåŠ¨æœåŠ¡å‘ç°å’ŒæŒ‡æ ‡é‡‡é›†ã€‚
- **é…ç½®**ï¼šé€šè¿‡ `values.yaml` ä¸­ `scheduler.serviceMonitor.enabled` å’Œ `devicePlugin.serviceMonitor.enabled` å¼€å…³æ§åˆ¶ã€‚

#### FEAT-4ï¼šæŒ‡æ ‡æ–°å¢ `device_type` æ ‡ç­¾ï¼ˆ#1612ï¼‰
- **æ–‡ä»¶**ï¼š`cmd/scheduler/metrics.go`
- **å˜æ›´**ï¼šè°ƒåº¦å™¨æŒ‡æ ‡ä¸­æ‰€æœ‰ä¸è®¾å¤‡ç›¸å…³çš„ Gauge/Counter æ–°å¢ `device_type` labelï¼Œä¾¿äºåœ¨ Grafana ä¸­æŒ‰è®¾å¤‡ç±»å‹ï¼ˆNVIDIA/Ascend/Cambricon ç­‰ï¼‰åˆ†ç»„å±•ç¤ºã€‚

#### FEAT-5ï¼švGPUmonitor æ”¯æŒè‡ªå®šä¹‰ `metrics-bind-address`ï¼ˆ#1613ï¼‰
- **æ–‡ä»¶**ï¼š`cmd/vGPUmonitor/main.go`
- **å˜æ›´**ï¼šæ–°å¢ `--metrics-bind-address` å‘½ä»¤è¡Œå‚æ•°ï¼Œå…è®¸è‡ªå®šä¹‰ç›‘æ§ç«¯ç‚¹ç›‘å¬åœ°å€ï¼Œä¸å†ç¡¬ç¼–ç  `:9394`ã€‚

#### FEAT-6ï¼š`checkUUID` æå–ä¸ºå…¬å…±å‡½æ•°å¤ç”¨ï¼ˆ#1622ï¼‰
- **æ–‡ä»¶**ï¼š`pkg/device/devices.go`ï¼ˆæ–°å¢ï¼‰ã€`pkg/device/nvidia/device.go`ï¼ˆåˆ é™¤ç§æœ‰æ–¹æ³•ï¼‰
- **å˜æ›´**ï¼šå°† NVIDIA ç§æœ‰çš„ `checkUUID()` é‡æ„ä¸ºåŒ…çº§å…¬å…±å‡½æ•° `device.CheckUUID()`ï¼Œå…¶ä»–è®¾å¤‡å‚å•†å®ç°å¯ç›´æ¥å¤ç”¨ UUID ç™½åå•/é»‘åå•è¿‡æ»¤é€»è¾‘ã€‚
```go
// ä» nvidia ç§æœ‰æ–¹æ³•æå‡ä¸º device åŒ…å…¬å…±å‡½æ•°
func CheckUUID(annos map[string]string, deviceID, useUUIDAnno, noUseUUIDAnno, commonWord string) bool
```

---

### âš¡ æ€§èƒ½ä¼˜åŒ–

#### PERF-1ï¼šNodeLock é‡è¯•ç­–ç•¥æ”¹ç”¨æŒ‡æ•°é€€é¿ï¼ˆ#1663ï¼‰
- **æ–‡ä»¶**ï¼š`pkg/util/nodelock/nodelock.go`
- **å˜æ›´**ï¼š`DefaultStrategy` ä¸­ `Factor` ä» `1.0`ï¼ˆçº¿æ€§ï¼‰æ”¹ä¸º `2.0`ï¼ˆæŒ‡æ•°ï¼‰ï¼Œ`Jitter` ä» `0.1` æ”¹ä¸º `0.5`ï¼ˆæ›´å¤§éšæœºæŠ–åŠ¨ï¼‰ï¼Œå‡å°‘é«˜å¹¶å‘åœºæ™¯ä¸‹å¤šè°ƒåº¦å™¨å®ä¾‹äº‰æŠ¢èŠ‚ç‚¹é”æ—¶çš„æƒŠç¾¤æ•ˆåº”ã€‚
```go
DefaultStrategy = wait.Backoff{
    Steps:    5,
    Duration: 100 * time.Millisecond,
    Factor:   2.0,  // æŒ‡æ•°é€€é¿ï¼š100ms â†’ 200ms â†’ 400ms â†’ 800ms â†’ 1600ms
    Jitter:   0.5,  // Â±50% éšæœºæŠ–åŠ¨
}
```

---

### ğŸ“¦ ä¾èµ–å‡çº§

| ä¾èµ– | æ—§ç‰ˆæœ¬ | æ–°ç‰ˆæœ¬ |
|------|--------|--------|
| `google.golang.org/grpc` | 1.78.0 | 1.79.1 |
| `github.com/NVIDIA/k8s-device-plugin` | â€” | æœ€æ–° |
| `github.com/NVIDIA/nvidia-container-toolkit` | â€” | æœ€æ–° |
| `golang.org/x/tools` | 0.41.0 | 0.42.0 |
| `github.com/onsi/gomega` | 1.39.0 | 1.39.1 |
| `github.com/onsi/ginkgo/v2` | 2.27.5 | 2.28.1 |
| `docker/build-push-action` (CI) | 6.18.0 | 6.19.2 |
| `docker/login-action` (CI) | 3.6.0 | 3.7.0 |
| `aquasecurity/trivy-action` (CI) | 0.33.1 | 0.34.1 |

---

### ğŸ§ª æµ‹è¯•è¦†ç›–å¢å¼º

| æ–°å¢æµ‹è¯•æ–‡ä»¶/å‡½æ•° | è¦†ç›–åœºæ™¯ |
|---|---|
| `pkg/scheduler/routes/route_test.go` | `LimitReader` è¶…å¤§è¯·æ±‚ä½“è§¦å‘ EOF |
| `pkg/scheduler/webhook_test.go` `TestSchedulerNameEmptyNoOverwrite` | `ForceOverwriteDefaultScheduler=false` æ—¶è°ƒåº¦å™¨åç§°æ³¨å…¥ |
| `pkg/util/leaderelection/leaderelection_test.go` | nil callbackã€nil leaseã€nil HolderIdentityã€nil LeaseDurationSeconds å…¨åœºæ™¯ |
| `pkg/device/ascend/device_test.go` | 910C å¥‡æ•°è¯·æ±‚æ‹’ç»ã€æ¨¡å—å¯¹å„¿åˆ†é…ã€`computeBestCombination910C` |